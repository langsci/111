\chapter{Multilingual Machine Translation}
\label{chapter12}
\setcounter{enums}{0}


\noindent Using information structure can improve multilingual machine
translation.  The main contribution that using information structure
makes is that a machine translation system informed by information
structure is capable of reducing the number of infelicitous
translations dramatically. This reduction has two effects on the
performance of \isi{transfer-based} machine translation
\citep{song:bender:11}: First, the processing burden of the machine
translation component which ranks the translations and selects only
suitable results can be greatly lightened, which should improve
translation speed.  Second, although it is still necessary to employ a
re-ranking model for choosing translations, we can start from a
refined set of translations, which should improve translation
accuracy.


\myS{2:sec:tmt} goes over the basis of transfer-based machine
translation.  \myS{12:sec:machinery} offers an explanation of how
\isi{ICONS} (Individual CONStraints)\is{individual constraints}
operates in \isi{transfer-based} machine
translation. \myS{12:sec:processor} addresses the processor the
current work employs for testing machine
translation. \myS{13:sec:evaluation} conducts an evaluation to examine
how many infelicitous translations are filtered out by means of ICONS.



\section{Transfer-based Machine Translation}
\label{2:sec:tmt}


The basic method I employ for testing machine translation herein is
built on the symbolic approach to machine translation, which normally
consists of three stages:\is{transfer-based}\is{generation} namely (i)
parsing, (ii) transfer, and (iii) generation.  Since MRS is not an
interlingua\is{MRS} (i.e.\ a meaning representation language in which
the representations are identical for all languages), using MRS for
machine translation requires an independent stage to convert one MRS
into another MRS. This stage is called transfer, and is carried out
between parsing and generation.  \is{transfer-based}\is{generation}




Figure~\ref{fig:mt}, adapted from \citet{oepen:etal:07} and
\citet{song:etal:10},\is{MRS} is illustrative of the MRS-based
architecture of machine translation. The first step (i.e.\ parsing)
analyses a sentence with a computational grammar for the source
language, whose output is a form of semantic representation such as a
(near) logical form.  The output of the first step serves as the
source of the next step (i.e.\ transfer), which is called MRS$_s$
(i.e.\ an input MRS).  The transfer module converts the source
representation obtained from the parsing process into another type of
representation compatible with the target language, which is called
MRS$_t$ (i.e.\ an output MRS).  MRS$_t$ is used as the source for the
final step (i.e.\ generation), which constructs one or more surface
form(s) built from the semantic representation. As a consequence, the
two surface forms in the source language and the target language are
compatible with a common meaning representation.





\begin{figure}[!t]
\begin{center} 
\includegraphics[width=.9\textwidth]{pdf/mt.pdf}
\caption{HPSG/MRS-based MT architecture}
\label{fig:mt}
\end{center}
\end{figure}


\section{Basic Machinery}
\label{12:sec:machinery}

A graph presented in (\ref{fig:mmt:basic}a) represents an \ili{English}
sentence in which the subject \textit{the \textsc{dog}} bears the
A-accent, thereby plays the role of \tdl{semantic-focus}.\is{semantic focus}
The second graph in (\ref{fig:mmt:basic}b) represents the \ili{Japanese}
translation, in which the subject \textit{inu} `dog' is combined with
the nominative marker \ga that signals \tdl{non-topic}.  That is to
say, although the two sentences provided in (\ref{fig:mmt:basic}a-b)
are proper translations of each other, information is differently
structured.

\myexe{\enumsentence{\toplabel{fig:mmt:basic}
\begin{tabular}[t]{lllll}
a. & \evnup{\includegraphics{pdf/basic1-eng.pdf}} & \xspace &
b. & \evnup{\includegraphics{pdf/basic1-jpn.pdf}} \\
\end{tabular}}}



\noindent Note that \tdl{non-topic} is a supertype of
\tdl{semantic-focus} in the type hierarchy of \tdl{info-str} given in
Figure~\ref{fig:info-str}
\mypage{fig:info-str}.\is{\textit{info-str}}\is{semantic focus}  
This ability to partially specify information
structure allows us to reduce the range of outputs in translation
while still capturing all legitimate possibilities.



Two hypothetical suffixes `-a' and `-b' are employed for testing
hereafter, and they represent the A and B accents in \ili{English}
\citep{bolinger:61,jackendoff:72} respectively.  Note that the `-b'
suffix cannot be attached to the verb \textit{barks}, because verbs
presumably cannot be marked via B-accent for the information
structure role of \tdl{topic} in English.\is{topic}  \textit{The dog barks}
without any information structure marking logically can be interpreted
as six types of sentences (3\ensuremath{\times}2).\is{A-accent}\is{B-accent}




\myexe{\enumsentence{\label{exe:eng}
\begin{tabular}[t]{ll}
dog & dog: [ ICONS: \ensuremath{<} \ensuremath{>} ] \\
    & dog-a: [ ICONS: \ensuremath{<} e2 \textbf{semantic-focus} x4 \ensuremath{>} ]\\
    & dog-b: [ ICONS: \ensuremath{<} e2 \textbf{contrast-or-topic} x4 \ensuremath{>} ]\\
bark &  barks: [ ICONS: \ensuremath{<} \ensuremath{>} ] \\
    & barks-a: [ ICONS: \ensuremath{<} e2 \textbf{semantic-focus} e2 \ensuremath{>} ]
\end{tabular}}}


\noindent However, if we apply \isi{ICONS} to generation, we can
filter out sentences which are not equivalent to the input sentence
with respect to information structure. For example, if the input
sentences are \textit{The \textsc{dog} barks} and \textit{The
  \textbf{dog} barks} in which the subject bears the A and B accents
respectively, they can be monolingually paraphrased as
(\ref{exe:para:eng}). That is, four infelicitous sentences from each
set of sentences can be removed.  Two sentences in
(\ref{exe:para:eng}a-i) and (\ref{exe:para:eng}a-iii) cannot be
generated because the subject does not include any value in ICONS. In
other words, information structure-marked constituents in the source
cannot be generated as an unmarked constituent in the target.  Two
sentences in (\ref{exe:para:eng}a-v) and (\ref{exe:para:eng}a-vi)
cannot be generated, either.\is{B-accent} This is because the
B-accented subject conveys \tdl{contrast-or-topic} which is
incompatible with \tdl{semantic-focus}.\is{semantic focus}\is{contrastive focus}
The same goes for (\ref{exe:para:eng}b): Since only the last two sentences are
compatible with the information structure meaning that the input
sentence conveys, the others cannot be paraphrased with respect to
information structure.





\myexe{\enumsentence{\label{exe:para:eng}
\begin{tabular}[t]{ll}
a. & The dog-\textbf{a} barks [ ICONS: \ensuremath{<} e2 \textbf{semantic-focus} x4 \ensuremath{>} ]\\
 & \xtab(i) \Midline{The dog barks}\\
 & \xtab(ii) The dog-a barks\\
 & \xtab(iii) \Midline{The dog barks-a}\\
 & \xtab(iv) The dog-a barks-a\\
 & \xtab(v) \Midline{The dog-b barks}\\
 & \xtab(vi) \Midline{The dog-b barks-a}\\
\end{tabular}}}

\myexe{\enumsentence[]{
\begin{tabular}[t]{ll}
b. & The dog-\textbf{b} barks [ ICONS: \ensuremath{<} e2 \textbf{contrast-or-topic} x4 \ensuremath{>} ]\\
 & \xtab(i) \Midline{The dog barks}\\
 & \xtab(ii) \Midline{The dog-a barks}\\
 & \xtab(iii) \Midline{The dog barks-a}\\
 & \xtab(iv) \Midline{The dog-a barks-a}\\
 & \xtab(v) The dog-b barks\\
 & \xtab(vi) The dog-b barks-a\\
\end{tabular}}}


\noindent The same goes for Japanese in which lexical markers signal
information structure.\is{lexical markers} There are at least three Japanese translations
(i.e.\ case-marking, \wa-marking, and null-marking) corresponding to
\textit{The dog barks}, but case-marked NPs cannot be paraphrased into
\wa-marked NPs within our \tdl{info-str} hierarchy given in
Figure~\ref{fig:info-str},\is{\textit{info-str}} and vice versa. Note that null-marked items in
Japanese (e.g.\ \textit{inu} in (\ref{exe:para:jpn}a-iii) and
(\ref{exe:para:jpn}b-iii)) are assigned \tdl{non-focus}
\citep{yatabe:99}, which is compatible with both \tdl{non-topic} in
(\ref{exe:para:jpn}a) and \tdl{contrast-or-topic}
(\ref{exe:para:jpn}b). Thus, \textit{inu-ga}/\textit{wa hoeru} can be
paraphrased into \textit{inu hoeru}.





\myexe{\enumsentence{\label{exe:para:jpn}
\begin{tabular}[t]{ll}
a. & inu \textbf{ga} hoeru [ ICONS: \ensuremath{<} e2 \textbf{non-topic} x4 \ensuremath{>} ]\\
 & \xtab(i) inu ga hoeru\\
 & \xtab(ii) \Midline{inu wa hoeru}\\
 & \xtab(iii) inu hoeru\\
b. & inu \textbf{wa} hoeru [ ICONS: \ensuremath{<} e2 \textbf{contrast-or-topic} x4 \ensuremath{>} ]\\
 & \xtab(i) \Midline{inu ga hoeru}\\
 & \xtab(ii) inu wa hoeru\\
 & \xtab(iii) inu hoeru\\
\end{tabular}}}


Translating across languages is constrained in the same manner. An
\ili{English} sentence (\ref{exe:tran}a) cannot be translated into
(\ref{exe:tran}a-ii) and (\ref{exe:tran}a-iii), because the
\tdl{semantic-focus} role that \textsc{dog} involves is incompatible
with the \tdl{contrast-or-topic} role that \textit{wa} assigns and the
\tdl{non-focus} role that the null marker (indicated by {\O})
involves. On the other hand, a Japanese sentence (\ref{exe:tran}b) can
be translated into only (\ref{exe:tran}b-ii) and (\ref{exe:tran}b-iv),
because \tdl{non-topic}, which comes from the nominative marker
\textit{ga},\is{B-accent} is contradictory to \tdl{contrast-or-topic} that the
B-accent signals in English and the constituent corresponding to the
\ga-marked subject should introduce an \tdl{info-str} element into
ICONS.



\myexe{\enumsentence{\label{exe:tran}
\begin{tabular}[t]{ll}
a. & The dog-\textbf{a} barks [ ICONS: \ensuremath{<} e2 \textbf{semantic-focus} x4 \ensuremath{>} ]\\
 & \xtab(i) inu ga hoeru\\
 & \xtab(ii) \Midline{inu wa hoeru}\\
 & \xtab(iii) \Midline{inu hoeru}\\
b. & inu \textbf{ga} hoeru [ ICONS: \ensuremath{<} e2 \textbf{non-topic} x4 \ensuremath{>} ]\\
 & \xtab(i) \Midline{The dog barks}\\
 & \xtab(ii) The dog-a barks\\
 & \xtab(iii) \Midline{The dog barks-a}\\
 & \xtab(iv) The dog-a barks-a\\
 & \xtab(v) \Midline{The dog-b barks}\\
 & \xtab(vi) \Midline{The dog-b barks-a}\\
\end{tabular}}}




\section{Processor}
\label{12:sec:processor}

The processor the present work uses for the purpose of evaluation is
\isi{\ace}.\footnote{\isi{\ace} (\myurl{http://sweaglesw.org/linguistics/ace})
is the first \isi{DELPH-IN} processor to specifically handle
\isi{ICONS} as part of the MRS, and then \isi{\agree}~(\citealt{slayden:12}) also uses 
   \isi{ICONS} for constraining information structure in parsing and
  generation.}.
\isi{\ace}, using \isi{DELPH-IN} grammars, such as \lingo \isi{Grammar Matrix} grammars created by
the customization system
\citep{bender:flickinger:05,drellishak:09,bender:etal:10} and resource
grammars (e.g.\ the ERG (English Resource Grammar,
\citealt{flickinger:00})), parses the sentences of natural languages,
and generates sentences based on the MRS\is{MRS} (Minimal Recursion Semantics,
\citealt{copestake:etal:05}) representation that the parser creates. 




When creating the data file of \isi{\ace}, \ace refers to parameters
described in \textit{ace/config.tdl}. In the configuration file,
grammar users can choose whether or not \isi{ICONS} is used in MRS
representation. The snippet that enables \isi{ICONS} to be included in MRS
representation is as follows.

\myexe{\enumsentence{\toplabel{tdl:config}
\evnup{\includegraphics{pdf/tdl-config.pdf}}}}


\isi{\ace} carries out ICONS-based generation via subsumption check,
using the type hierarchy \tdl{info-str} (presented in
Figure~\ref{fig:info-str} \mypage{fig:info-str}).\is{\textit{info-str}}
\isi{\ace} generates all potential sentences that logically fit in the
input MRS not considering the constraints on \isi{ICONS}
beforehand. After that, if the data file of the grammar for generation
is compiled with the parameters given in \myref{tdl:config},
\isi{\ace} starts postprocessing the intermediate results.  Depending
on the subsumption relationship of information structure meanings,
sentences mismatching the values in the \isi{ICONS} list are filtered
out in this step.  For example, if \tdl{semantic-focus} is assigned to
a specific individual in the source MRS,\is{MRS} only outputs that
provide an \isi{ICONS} element for that individual can be
produced.\is{\textit{info-str}} The \tdl{info-str} value an individual
has in the output should be the same as that in the input
(i.e.\ \tdl{semantic-focus})\is{semantic focus} or its supertypes (e.g.\ \tdl{focus},
\tdl{non-topic}, etc.). For instance, an A-accented constituent in
\ili{English} (e.g.\ \textsc{dog}) contributes an \isi{ICONS} element whose
value is \tdl{semantic-focus}, and this element is translated as a
\ga-marked constituent (e.g.\ \textit{inu ga}) whose value is
monolingually \tdl{non-topic} in \ili{Japanese}. Note that
\tdl{non-topic} subsumes \tdl{semantic-focus} in the type hierarchy
presented in Figure~\ref{fig:info-str}.\is{\textit{info-str}} A
completely underspecified output for each \isi{ICONS} element is not
acceptable in generation.\is{underspecification}
For instance, an A-accented \textsc{dog}
that introduces an \isi{ICONS} element cannot be paraphrased as an
unaccented \textit{dog} that does not contribute any \isi{ICONS}
element.\is{A-accent} By contrast, the opposite direction is
acceptable.  If a constituent introduces no \isi{ICONS} element in the
input, the output can include an information-structure marked
constituent. For instance, an unaccented \textit{dog} can be
paraphrased as an A-accented \textsc{dog} in generation.






\section{Evaluation}
\label{13:sec:evaluation}


\subsection{Illustrative Grammars}
\label{13:ssec:grammars}

In order to verify a linguistic hypothesis with reference to a
computational grammar,\is{illustrative grammars} it is a good strategy
to use a compact grammar presenting the fundamental rules in a precise
manner. Illustrative grammars are constructed for this purpose.  The
illustrative languages used here are \ili{English}, \ili{Japanese}, and
\ili{Korean}.  These languages are chosen, because the resource
grammars for each of the language (\isi{ERG} (English Resource
Grammar, \citealt{flickinger:00}) for English, \isi{Jacy}
(\citealt{siegel:bender:02}) for \ili{Japanese}, and \isi{KRG} (Korean
Resource Grammar, \citealt{kim:etal:11}) for Korean) will be the main
concern in my further study.  The information structure properties
each language has are summarized in the following subsections.



\subsubsection{English}
\label{13:sssec:eng}


As is well-known,\is{prosody} \ili{English} employs prosody for expressing
information structure.  Without consideration of the prosodic
patterns, we could not draw the basic picture of information-structure
related phenomena in English.\footnote{English also makes use of some
  constructional means to configure focus and \isi{topic}. These include
  \isi{focus}/topic \isi{fronting}, \isi{clefting}, etc. Nonetheless, these
  have to do with various grammatical components. For example,
  implementing grammatical modules for cleft constructions
  necessitates many \isi{TDL} statements for relative clauses as an
  essential prerequisite.\is{relative clause} This involves too much
  complexity for an illustrative grammar to cover.  For this reason,
  the illustrative grammar for English in this evaluation is
  exclusively concerned with prosody.} There are quite a few previous
studies on how prosody is realized with respect to information
structure
\citep{jackendoff:72,steedman:00,kadmon:01,buring:03,hedberg:06}, but
there seems to be no clear consensus (as surveyed earlier in
\myS{4:sec:prosody}).\is{illustrative grammars} The illustrative
grammar for English makes use of just the traditional distinction of
the A and B accents \citep{bolinger:58}. In order to articulate them
as a string for ease of exposition, the two hypothetical suffixes `-a'
and `-b' are used. However, the meanings that the accents take charge
of are represented differently from the traditional approach. The
information structure meanings that `-a' and `-b' convey are marked
following \citeauthor{hedberg:06}'s argument: `-a' for
\tdl{semantic-focus} and `-b' for \tdl{contrast-or-topic}.\is{semantic focus}
The AVMs are already presented in \myS{9:ssec:eng} \mypage{avm:fc:tp:eng}.



\subsubsection{Korean}
\label{13:ssec:kor}


The illustrative grammar for \ili{Korean} includes two kinds of
grammatical sets of constraints for expressing information
structure.\is{lexical markers} The first one employs lexical markers, such as \ika and
\lul for case marking, \nun for \isi{topic} marking, and {\O}~for null
marking. The AVMs for these markers are presented in
\myS{9:ssec:jpn:kor} \mypage{avm:ika:nun:kor}.  The second fragment
aims to handle scrambling. The AVMs for constraining scrambling
constructions are provided in
\myS{10:sec:scrambling} \mypage{avm:top:scr}.\is{scrambling} These AVMs use different
rules instantiating \tdl{head-subj-phrase} and \tdl{head-comp-phrase}
with reference to lexical markings of daughters (i.e.\ MKG).\is{MKG}


\subsubsection{Japanese}
\label{13:ssec:jpn}



As mentioned before,\is{HPSG} the present study respects the
traditional ways of dealing with lexical markers in \ili{Japanese} and
\ili{Korean} from different points of view. While lexical markers in
Korean are dealt with as suffixes \citep{kim:yang:04}, those in
Japanese are treated as adpositions \citep[among
  others]{siegel:99}.\is{adposition}\is{illustrative grammars} Other
than this difference, the illustrative grammar for Japanese has the
same configuration as that for Korean explained above. Notably, the
null marker in Japanese is constrained by a lexical rule in the
current work \mypage{avm:ga:wa:jpn}, which is different from previous
HPSG-based suggestion about so-called case-ellipsis
\citep{yatabe:99,sato:tam:12}.




\subsection{Testsuites}
\label{13:ssec:testsuites}


The \isi{testsuites} (a collection of sentence to be modeled) for this
multilingual machine translation testing are provided in
(\ref{exe:testsuite:eng}-\ref{exe:testsuite:kor}); \ili{English}, \ili{Japanese},
and \ili{Korean}, respectively.  There is one intransitive sentence and one
transitive sentence in English, and they are encoded with two
hypothetical suffixes and differentiated as allosentences.



\myexe{\enumsentence{\label{exe:testsuite:eng}
\evnup{\includegraphics[scale=0.9]{pdf/item-eng.pdf}}}}

\myexe{\enumsentence{\label{exe:testsuite:jpn}
\evnup{\includegraphics[scale=0.9]{pdf/item-jpn.pdf}}}}

\myexe{\enumsentence{\label{exe:testsuite:kor}
\evnup{\includegraphics[scale=0.9]{pdf/item-kor.pdf}}}}



\subsection{An Experiment}
\label{12:ssec:experiments}


\begin{table}[!t]
\small
\centering
\begin{tabular}{ccc}
Table 13.1: \# of outputs without \isi{ICONS} & \mbox{ } &
Table 13.2: \# of outputs with ICONS\\ 
\includegraphics{pdf/tbl_total-number-of-output2.pdf} & & 
\includegraphics{pdf/tbl_total-number-of-output.pdf} \\
\end{tabular}
\end{table}


\begin{figure}[!t]
\begin{center} 
\includegraphics[width=.9\textwidth]{pdf/comparison2.pdf}
\caption{Average \# of outputs}
\label{fig:comp:mmt}
\end{center}
\end{figure}

\noindent All test items presented in \myref{exe:testsuite:eng} and
their translations in \ili{Japanese} and \ili{Korean} are parsed,
transferred, and generated.\is{transfer-based}\is{generation}  Table 13.1 and Table 13.2 show the number
of translation results in each translation pair. The first column in
each table indicates the source language, and the first row indicates
the target language. For example, [English \ensuremath{\rightarrow}
Japanese] produces 126 translations when not using ICONS, and 39
translations when using ICONS.\is{testsuites}


As indicated in the tables, the number of generated sentences
dramatically decreases when using ICONS. The total number of
translation outputs in Table 15.1 is 3,222, while that in Table 15.2
is merely 960.\footnote{When the source language is not \ili{English} and
  the target language is English, the numbers are rather big in Table
  15.1. This is because English employs number and COG-ST features,
  while \ili{Japanese} and \ili{Korean} do not. For example,
  \textit{inu} in Japanese can be translated into at least four NP
  types in English: \textit{a dog}, \textit{the dog}, \textit{the
    dogs}, and \textit{dogs}.} That means approximately 70\% of the
translations are filtered out in total when using ICONS.



The four charts in Figure~\ref{fig:comp:mmt} compare the average
number of outputs in total and in each translation pair. The decrease
indicated in each bar chart shows that information structure can be
used to filter out inappropriate sentences from the translation
result.




These charts show that when translating \ili{Japanese} and
\ili{Korean} to \ili{English} many outputs are filtered out. The main reason
for the dramatic decrease in [Japanese \ensuremath{\rightarrow} English]
and [Korean \ensuremath{\rightarrow} English] is that the illustrative
grammar for English includes a lexical rule to mark \isi{focus} on verbal
items, while the illustrative grammars for Japanese and Korean do
not.\is{illustrative grammars} When a verb is focused in the English grammar, 
the lexical rule introduces a \tdl{focus} element into ICONS.\is{\textit{info-str}}
In contrast, verbs cannot involve any \tdl{info-str} 
value in the current illustrative grammars
for Japanese and Korean. Thus, the huge difference in [Japanese
\ensuremath{\rightarrow} English] and [Korean \ensuremath{\rightarrow}
English] is largely caused by the different marking system for verbal
items.


Finally, I verified 108 sets of translation outputs (9 directions
\ensuremath{\times} 12 test items) by hand. The same problem was also
found. When an \ili{English} item includes an A-accented verb
(e.g.\ \textit{barks-a} and \textit{reads-a}),\is{A-accent} the item
cannot be translated into \ili{Japanese} and \ili{Korean}. This
suggest that there might be a problem with the strategy of requiring
some information structure marking in the output for an item if there
is some in the input.  Other than this difference, the translation
outputs were legitimate and felicitous.  I also sampled the filtered
translations to verify that they were all infelicitous ones, and found
that this information structure-based model works for them, too.




\section{Summary}
\label{13:sec:sum}

It is my firm opinion that translating should mean reshaping the ways
in which information is conveyed, not simply changing words and
reordering phrases.  In almost all human languages, articulation of
sentential constituents is conditioned by information
structure. Because the means of expressing information structure
differs across languages, identifying how information is structured in
a given set of languages plays a key role in achieving felicity in a
machine translation between them.\is{felicity} Hence, information
structure is of great help to multilingual machine translation in that
information structure facilitates more felicitous translations.  This
chapter conducted a small experiment to support this
hypothesis.\is{illustrative grammars} I created three illustrative
grammars for \ili{English}, \ili{Japanese}, and \ili{Korean} following my
ICONS-based analyses presented thus far. In the test of transfer-based
machine translation,\is{transfer-based} I found that using information
structure served to filter out infelicitous translations
dramatically. This testing should be further elaborated in future work
using resource grammars, such as the \isi{ERG}
(\citealt{flickinger:00}), \isi{Jacy} (\citealt{siegel:bender:02}),
and the \isi{KRG} (\citealt{kim:etal:11}).


